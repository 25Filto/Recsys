{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def local_kernel(u, v):\n",
    "    dist = torch.norm(u - v, p=2, dim=2)\n",
    "    hat = torch.clamp(1. - dist**2, min=0.)\n",
    "    return hat\n",
    "\n",
    "# 데이터 로딩\n",
    "import numpy as np\n",
    "users, movies, ratings = [], [], []\n",
    "user_len, movie_len = 0, 0\n",
    "\n",
    "with open(\"../data/ratings.dat\", \"r\") as f:\n",
    "    for line in f:\n",
    "        u, m, r, t = line.split(\"::\")\n",
    "        user_len = max(user_len, int(u))\n",
    "        movie_len = max(movie_len, int(m))\n",
    "        users.append(int(u))\n",
    "        movies.append(int(m))\n",
    "        ratings.append(float(r))\n",
    "\n",
    "indices = np.arange(len(ratings))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "users = np.array(users)[indices]\n",
    "movies = np.array(movies)[indices]\n",
    "ratings = np.array(ratings)[indices]\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "users = torch.tensor(users, dtype=torch.long, device=device)\n",
    "movies = torch.tensor(movies, dtype=torch.long, device=device)\n",
    "ratings = torch.tensor(ratings, dtype=torch.float, device=device)\n",
    "\n",
    "# 데이터 분할\n",
    "n_samples = len(ratings)\n",
    "train_size = int(0.6 * n_samples)\n",
    "valid_size = int(0.2 * n_samples)\n",
    "\n",
    "users_train = users[:train_size]\n",
    "movies_train = movies[:train_size]\n",
    "ratings_train = ratings[:train_size]\n",
    "\n",
    "users_valid = users[train_size:train_size+valid_size]\n",
    "movies_valid = movies[train_size:train_size+valid_size]\n",
    "ratings_valid = ratings[train_size:train_size+valid_size]\n",
    "\n",
    "users_test = users[train_size+valid_size:]\n",
    "movies_test = movies[train_size+valid_size:]\n",
    "ratings_test = ratings[train_size+valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rating_matrix(users, movies, ratings, num_users, num_items):\n",
    "    # 행렬 크기를 정확히 맞춤\n",
    "    rating_matrix = torch.zeros((num_users, num_items), dtype=torch.float32)\n",
    "    mask = torch.zeros_like(rating_matrix)\n",
    "\n",
    "    for u, m, r in zip(users, movies, ratings):\n",
    "        rating_matrix[u-1, m-1] = r  # 인덱스를 0부터 시작하도록 조정\n",
    "        mask[u-1, m-1] = 1.0\n",
    "    return rating_matrix.to(device), mask.to(device)\n",
    "\n",
    "n_users = user_len\n",
    "n_items = movie_len\n",
    "\n",
    "train_r, train_m = make_rating_matrix(users_train, movies_train, ratings_train, n_users, n_items)\n",
    "valid_r, valid_m = make_rating_matrix(users_valid, movies_valid, ratings_valid, n_users, n_items)\n",
    "test_r, test_m = make_rating_matrix(users_test, movies_test, ratings_test, n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        # 입력 차원과 출력 차원을 정확히 맞춤\n",
    "        self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
    "        self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
    "        self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
    "        self.b = nn.Parameter(torch.randn(n_hid))\n",
    "\n",
    "        self.lambda_s = lambda_s\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.b)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 차원 확인\n",
    "        if x.size(1) != self.W.size(0):\n",
    "            # 차원이 맞지 않으면 조정\n",
    "            x = x[:, :self.W.size(0)]\n",
    "        \n",
    "        w_hat = local_kernel(self.u, self.v)\n",
    "        sparse_reg = F.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
    "        sparse_reg_term = self.lambda_s * sparse_reg\n",
    "        l2_reg = F.mse_loss(self.W, torch.zeros_like(self.W))\n",
    "        l2_reg_term = self.lambda_2 * l2_reg\n",
    "        W_eff = self.W * w_hat\n",
    "        y = torch.matmul(x, W_eff) + self.b\n",
    "        y = self.activation(y)\n",
    "        return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
    "            else:\n",
    "                layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
    "        # 마지막 레이어의 출력 차원을 입력 차원과 동일하게 설정\n",
    "        layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        total_reg = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, reg = layer(x)\n",
    "            if i < len(self.layers)-1:\n",
    "                x = self.dropout(x)\n",
    "            if total_reg is None:\n",
    "                total_reg = reg\n",
    "            else:\n",
    "                total_reg += reg\n",
    "        return x, total_reg\n",
    "class CompleteNet(nn.Module):\n",
    "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
    "        super().__init__()\n",
    "        self.gk_size = gk_size\n",
    "        self.dot_scale = dot_scale\n",
    "        self.local_kernel_net = kernel_net\n",
    "        # n_m+1 대신 n_m을 사용\n",
    "        self.global_kernel_net = KernelNet(n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2)\n",
    "        self.conv_kernel = nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
    "        nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "\n",
    "    def forward(self, train_r):\n",
    "        x, _ = self.local_kernel_net(train_r)\n",
    "        gk = self.global_kernel(x, self.gk_size, self.dot_scale)\n",
    "        x = self.global_conv(train_r, gk)\n",
    "        x, global_reg_loss = self.global_kernel_net(x)\n",
    "        # 출력 차원을 입력 차원과 동일하게 맞춤\n",
    "        if x.size(1) != train_r.size(1):\n",
    "            x = x[:, :train_r.size(1)]\n",
    "        return x, global_reg_loss\n",
    "\n",
    "    def global_kernel(self, input, gk_size, dot_scale):\n",
    "        avg_pooling = torch.mean(input, dim=0)\n",
    "        avg_pooling = avg_pooling.view(1, -1)\n",
    "        gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale\n",
    "        gk = gk.view(1, 1, gk_size, gk_size)\n",
    "        return gk\n",
    "\n",
    "    def global_conv(self, input, W):\n",
    "        input = input.unsqueeze(0).unsqueeze(0)\n",
    "        conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
    "        return conv2d.squeeze(0).squeeze(0)\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
    "        # 차원이 맞지 않으면 조정\n",
    "        if pred_p.size(1) != train_r.size(1):\n",
    "            pred_p = pred_p[:, :train_r.size(1)]\n",
    "        if train_m.size(1) != train_r.size(1):\n",
    "            train_m = train_m[:, :train_r.size(1)]\n",
    "            \n",
    "        diff = train_m * (train_r - pred_p)\n",
    "        sqE = F.mse_loss(diff, torch.zeros_like(diff))\n",
    "        loss_p = sqE + reg_loss\n",
    "        return loss_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.3620 | Valid Loss: 0.1030\n",
      "[Epoch 2] Train Loss: 0.3151 | Valid Loss: 0.0884\n",
      "[Epoch 3] Train Loss: 0.2711 | Valid Loss: 0.0737\n",
      "[Epoch 4] Train Loss: 0.2270 | Valid Loss: 0.0590\n",
      "[Epoch 5] Train Loss: 0.1831 | Valid Loss: 0.0450\n",
      "[Epoch 6] Train Loss: 0.1421 | Valid Loss: 0.0325\n",
      "[Epoch 7] Train Loss: 0.1057 | Valid Loss: 0.0224\n",
      "[Epoch 8] Train Loss: 0.0767 | Valid Loss: 0.0155\n",
      "[Epoch 9] Train Loss: 0.0568 | Valid Loss: 0.0118\n",
      "[Epoch 10] Train Loss: 0.0463 | Valid Loss: 0.0113\n",
      "[Epoch 11] Train Loss: 0.0448 | Valid Loss: 0.0127\n",
      "[Epoch 12] Train Loss: 0.0494 | Valid Loss: 0.0150\n",
      "[Epoch 13] Train Loss: 0.0552 | Valid Loss: 0.0169\n",
      "[Epoch 14] Train Loss: 0.0608 | Valid Loss: 0.0179\n",
      "[Epoch 15] Train Loss: 0.0642 | Valid Loss: 0.0179\n",
      "[Epoch 16] Train Loss: 0.0635 | Valid Loss: 0.0171\n",
      "[Epoch 17] Train Loss: 0.0601 | Valid Loss: 0.0156\n",
      "[Epoch 18] Train Loss: 0.0557 | Valid Loss: 0.0139\n",
      "[Epoch 19] Train Loss: 0.0510 | Valid Loss: 0.0123\n",
      "[Epoch 20] Train Loss: 0.0459 | Valid Loss: 0.0110\n",
      "[Epoch 21] Train Loss: 0.0421 | Valid Loss: 0.0100\n",
      "[Epoch 22] Train Loss: 0.0397 | Valid Loss: 0.0095\n",
      "[Epoch 23] Train Loss: 0.0382 | Valid Loss: 0.0093\n",
      "[Epoch 24] Train Loss: 0.0379 | Valid Loss: 0.0094\n",
      "[Epoch 25] Train Loss: 0.0381 | Valid Loss: 0.0096\n",
      "[Epoch 26] Train Loss: 0.0390 | Valid Loss: 0.0098\n",
      "[Epoch 27] Train Loss: 0.0398 | Valid Loss: 0.0100\n",
      "[Epoch 28] Train Loss: 0.0402 | Valid Loss: 0.0100\n",
      "[Epoch 29] Train Loss: 0.0403 | Valid Loss: 0.0100\n",
      "[Epoch 30] Train Loss: 0.0402 | Valid Loss: 0.0097\n",
      "[Epoch 31] Train Loss: 0.0393 | Valid Loss: 0.0094\n",
      "[Epoch 32] Train Loss: 0.0385 | Valid Loss: 0.0091\n",
      "[Epoch 33] Train Loss: 0.0375 | Valid Loss: 0.0088\n",
      "[Epoch 34] Train Loss: 0.0364 | Valid Loss: 0.0085\n",
      "[Epoch 35] Train Loss: 0.0354 | Valid Loss: 0.0083\n",
      "[Epoch 36] Train Loss: 0.0349 | Valid Loss: 0.0082\n",
      "[Epoch 37] Train Loss: 0.0342 | Valid Loss: 0.0082\n",
      "[Epoch 38] Train Loss: 0.0343 | Valid Loss: 0.0083\n",
      "[Epoch 39] Train Loss: 0.0343 | Valid Loss: 0.0084\n",
      "[Epoch 40] Train Loss: 0.0347 | Valid Loss: 0.0085\n",
      "[Epoch 41] Train Loss: 0.0349 | Valid Loss: 0.0086\n",
      "[Epoch 42] Train Loss: 0.0351 | Valid Loss: 0.0086\n",
      "[Epoch 43] Train Loss: 0.0351 | Valid Loss: 0.0086\n",
      "[Epoch 44] Train Loss: 0.0351 | Valid Loss: 0.0086\n",
      "[Epoch 45] Train Loss: 0.0349 | Valid Loss: 0.0085\n",
      "[Epoch 46] Train Loss: 0.0346 | Valid Loss: 0.0084\n",
      "[Epoch 47] Train Loss: 0.0343 | Valid Loss: 0.0083\n",
      "[Epoch 48] Train Loss: 0.0340 | Valid Loss: 0.0082\n",
      "[Epoch 49] Train Loss: 0.0337 | Valid Loss: 0.0081\n",
      "[Epoch 50] Train Loss: 0.0336 | Valid Loss: 0.0081\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6039x3951 and 3952x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m pretrained_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     39\u001b[0m optimizer_finetune\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m pred_ratings, reg_loss \u001b[38;5;241m=\u001b[39m pretrained_model(train_r)\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_ratings, reg_loss, train_m, train_r)\n\u001b[1;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[148], line 75\u001b[0m, in \u001b[0;36mCompleteNet.forward\u001b[0;34m(self, train_r)\u001b[0m\n\u001b[1;32m     73\u001b[0m gk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_kernel(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgk_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdot_scale)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_conv(train_r, gk)\n\u001b[0;32m---> 75\u001b[0m x, global_reg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_kernel_net(x)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 출력 차원을 입력 차원과 동일하게 맞춤\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m train_r\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[148], line 52\u001b[0m, in \u001b[0;36mKernelNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m total_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 52\u001b[0m     x, reg \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     54\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[148], line 31\u001b[0m, in \u001b[0;36mKernelLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m l2_reg_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_2 \u001b[38;5;241m*\u001b[39m l2_reg\n\u001b[1;32m     30\u001b[0m W_eff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m*\u001b[39m w_hat\n\u001b[0;32m---> 31\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x, W_eff) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     32\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(y)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sparse_reg_term \u001b[38;5;241m+\u001b[39m l2_reg_term\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6039x3951 and 3952x128)"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 설정\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "lambda_s = 1e-3\n",
    "lambda_2 = 1e-4\n",
    "num_epochs = 50\n",
    "\n",
    "# Pretrain 모델 초기화\n",
    "pretrain_model = KernelNet(n_items, n_hid, n_dim, n_layers, lambda_s, lambda_2).to(device)\n",
    "criterion = Loss()\n",
    "optimizer_kernel = optim.Adam(pretrain_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Pretrain 학습\n",
    "for epoch in range(num_epochs):\n",
    "    pretrain_model.train()\n",
    "    pred_train, reg_loss = pretrain_model(train_r)\n",
    "    loss = criterion(pred_train, reg_loss, train_m, train_r)\n",
    "\n",
    "    optimizer_kernel.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_kernel.step()\n",
    "\n",
    "    # Validation\n",
    "    pretrain_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_valid, _ = pretrain_model(valid_r)\n",
    "        val_loss = criterion(pred_valid, 0, valid_m, valid_r)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {loss.item():.4f} | Valid Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# CompleteNet 초기화 및 학습\n",
    "pretrained_model = CompleteNet(pretrain_model, n_u=n_users, n_m=n_items, n_hid=128, n_dim=10, n_layers=3, lambda_s=0.1, lambda_2=0.01, gk_size=4, dot_scale=1.0).to(device)\n",
    "optimizer_finetune = torch.optim.Adam(pretrained_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Fine-tuning\n",
    "for epoch in range(10):\n",
    "    pretrained_model.train()\n",
    "    optimizer_finetune.zero_grad()\n",
    "\n",
    "    pred_ratings, reg_loss = pretrained_model(train_r)\n",
    "    loss = criterion(pred_ratings, reg_loss, train_m, train_r)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_finetune.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
