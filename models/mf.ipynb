{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloworld\n"
     ]
    }
   ],
   "source": [
    "print(\"helloworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ml-1m/ratings.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m users, movies, ratings \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m      7\u001b[0m user_len, movie_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml-1m/ratings.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m         u, m, r, t \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-1m/ratings.dat'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. 먼저 전체 데이터를 하나의 리스트로 읽기\n",
    "users, movies, ratings = [], [], []\n",
    "user_len, movie_len = 0, 0\n",
    "\n",
    "with open(\"ml-1m/ratings.dat\", \"r\") as f:\n",
    "    for line in f:\n",
    "        u, m, r, t = line.split(\"::\")\n",
    "        user_len = max(user_len, int(u))\n",
    "        movie_len = max(movie_len, int(m))\n",
    "        users.append(int(u))\n",
    "        movies.append(int(m))\n",
    "        ratings.append(float(r))\n",
    "\n",
    "indices = np.arange(len(ratings))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "users = np.array(users)[indices]\n",
    "movies = np.array(movies)[indices]\n",
    "ratings = np.array(ratings)[indices]\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "users = torch.tensor(users, dtype=torch.long, device=\"mps\")\n",
    "movies = torch.tensor(movies, dtype=torch.long, device = \"mps\")\n",
    "ratings = torch.tensor(ratings, dtype=torch.float, device = \"mps\")\n",
    "\n",
    "# 3. 데이터 분할 (예: 60% 학습, 20% 검증, 20% 테스트)\n",
    "n_samples = len(ratings)\n",
    "train_size = int(0.6 * n_samples)\n",
    "valid_size = int(0.2 * n_samples)\n",
    "\n",
    "# 학습 데이터\n",
    "users_train = users[:train_size]\n",
    "movies_train = movies[:train_size]\n",
    "ratings_train = ratings[:train_size]\n",
    "\n",
    "\n",
    "# 검증 데이터\n",
    "users_valid = users[train_size:train_size+valid_size]\n",
    "movies_valid = movies[train_size:train_size+valid_size]\n",
    "ratings_valid = ratings[train_size:train_size+valid_size]\n",
    "\n",
    "\n",
    "# 테스트 데이터\n",
    "users_test = users[train_size+valid_size:]\n",
    "movies_test = movies[train_size+valid_size:]\n",
    "ratings_test = ratings[train_size+valid_size:]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200043])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(users_train, movies_train, ratings_train), batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(users_valid, movies_valid, ratings_valid), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(users_test, movies_test, ratings_test), batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a + b_i + b_u + r_u*r_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3114, grad_fn=<SumBackward0>)\n",
      "tensor([5.3114], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "user_emb = nn.Embedding(user_len+1, 100)\n",
    "item_emb = nn.Embedding(movie_len+1, 100)\n",
    "user_bias = nn.Embedding(user_len+1, 1)\n",
    "item_bias = nn.Embedding(movie_len+1, 1)\n",
    "alpha = ratings.float().mean()\n",
    "u1 = user_emb(torch.LongTensor([1]))\n",
    "i1 = item_emb(torch.LongTensor([1]))\n",
    "u_b = user_bias(torch.LongTensor([1]))\n",
    "i_b = item_bias(torch.LongTensor([1]))\n",
    "print(torch.sum(u1*i1))\n",
    "print(torch.sum(u1*i1, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, user_len, movie_len, ratings, rank):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_len = user_len\n",
    "        self.movie_len = movie_len\n",
    "        self.rank = rank\n",
    "        self.alpha = ratings.float().mean()\n",
    "        self.user_bias = nn.Embedding(user_len+1, 1)\n",
    "        self.item_bias = nn.Embedding(movie_len+1, 1)\n",
    "        self.user_reps = nn.Embedding(user_len+1, rank)\n",
    "        self.item_reps = nn.Embedding(movie_len+1, rank)\n",
    "    def forward(self, user,item):\n",
    "        user_bias = self.user_bias(user)\n",
    "        item_bias = self.item_bias(item)\n",
    "        user_emb = self.user_reps(user)\n",
    "        item_emb = self.item_reps(item)\n",
    "        return self.alpha + user_bias + item_bias + torch.sum(user_emb*item_emb)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmlab/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 45.9, Val Loss:17.7\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 1, Train Loss: 7.7, Val Loss:8.6\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 2, Train Loss: 3.6, Val Loss:5.9\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 3, Train Loss: 2.6, Val Loss:4.6\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 4, Train Loss: 2.2, Val Loss:3.8\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 5, Train Loss: 1.9, Val Loss:3.3\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 6, Train Loss: 1.7, Val Loss:3.0\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 7, Train Loss: 1.6, Val Loss:2.7\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 8, Train Loss: 1.5, Val Loss:2.5\n",
      "0번째 입니다\n",
      "10000번째 입니다\n",
      "20000번째 입니다\n",
      "30000번째 입니다\n",
      "40000번째 입니다\n",
      "50000번째 입니다\n",
      "60000번째 입니다\n",
      "70000번째 입니다\n",
      "80000번째 입니다\n",
      "90000번째 입니다\n",
      "100000번째 입니다\n",
      "110000번째 입니다\n",
      "120000번째 입니다\n",
      "130000번째 입니다\n",
      "140000번째 입니다\n",
      "150000번째 입니다\n",
      "160000번째 입니다\n",
      "170000번째 입니다\n",
      "180000번째 입니다\n",
      "190000번째 입니다\n",
      "200000번째 입니다\n",
      "210000번째 입니다\n",
      "220000번째 입니다\n",
      "230000번째 입니다\n",
      "240000번째 입니다\n",
      "250000번째 입니다\n",
      "260000번째 입니다\n",
      "270000번째 입니다\n",
      "280000번째 입니다\n",
      "290000번째 입니다\n",
      "300000번째 입니다\n",
      "310000번째 입니다\n",
      "320000번째 입니다\n",
      "330000번째 입니다\n",
      "340000번째 입니다\n",
      "350000번째 입니다\n",
      "360000번째 입니다\n",
      "370000번째 입니다\n",
      "380000번째 입니다\n",
      "390000번째 입니다\n",
      "400000번째 입니다\n",
      "410000번째 입니다\n",
      "420000번째 입니다\n",
      "430000번째 입니다\n",
      "440000번째 입니다\n",
      "450000번째 입니다\n",
      "460000번째 입니다\n",
      "470000번째 입니다\n",
      "480000번째 입니다\n",
      "490000번째 입니다\n",
      "500000번째 입니다\n",
      "510000번째 입니다\n",
      "520000번째 입니다\n",
      "530000번째 입니다\n",
      "540000번째 입니다\n",
      "550000번째 입니다\n",
      "560000번째 입니다\n",
      "570000번째 입니다\n",
      "580000번째 입니다\n",
      "590000번째 입니다\n",
      "600000번째 입니다\n",
      "Epoch: 9, Train Loss: 1.4, Val Loss:2.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "rank = 100\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model = MF(user_len, movie_len, ratings, rank).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses , valid_losses = [],[]    \n",
    "    model.train()\n",
    "    for i, (user, movie, rating) in enumerate(zip(users_train, movies_train, ratings_train)):\n",
    "        if(i%10000==0):\n",
    "            print(f\"{i}번째 입니다\")\n",
    "        rating = rating.unsqueeze(0)\n",
    "        pred = model(user, movie)\n",
    "        loss = criterion(pred, rating)\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for user, movie, rating in zip(users_valid, movies_valid, ratings_valid):\n",
    "        pred = model(user,movie)\n",
    "        loss = criterion(pred, rating)\n",
    "        valid_losses.append(loss.item())\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(valid_losses)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train Loss: {epoch_train_loss:0.1f}, Val Loss:{epoch_val_loss:0.1f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2708]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user = users_test[5].unsqueeze(0)\n",
    "    movie = movies_test[5].unsqueeze(0)\n",
    "    ratings = ratings_test[5].unsqueeze(0)\n",
    "    pred = model(user, movie)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200043])\n",
      "torch.Size([200043])\n",
      "torch.Size([200043])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5., 4., 3.,  ..., 4., 3., 5.], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(users_test.shape)\n",
    "print(movies_test.shape)\n",
    "print(ratings_test.shape)\n",
    "ratings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472유저가 2421에 3.0점을 줬다\n"
     ]
    }
   ],
   "source": [
    "print(f\"{users_test[4]}유저가 {movies_test[4]}에 {ratings_test[4]}점을 줬다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1472], device='mps:0')유저가 tensor([2421], device='mps:0')에 tensor([[2.0162]], device='mps:0')점을 줄 것이라 예측했다\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user = users_test[4].unsqueeze(0)\n",
    "    movie = movies_test[4].unsqueeze(0)\n",
    "    ratings = ratings_test[4].unsqueeze(0)\n",
    "    pred = model(user, movie)\n",
    "    print(f\"{user}유저가 {movie}에 {pred}점을 줄 것이라 예측했다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 89.8780, Val Loss: 66.8109\n",
      "Epoch: 2, Train Loss: 40.3435, Val Loss: 39.0168\n",
      "Epoch: 3, Train Loss: 17.4000, Val Loss: 24.1078\n",
      "Epoch: 4, Train Loss: 7.6721, Val Loss: 16.4039\n",
      "Epoch: 5, Train Loss: 3.7005, Val Loss: 12.3138\n",
      "Epoch: 6, Train Loss: 2.0348, Val Loss: 9.9884\n",
      "Epoch: 7, Train Loss: 1.3066, Val Loss: 8.6288\n",
      "Epoch: 8, Train Loss: 0.9745, Val Loss: 7.7759\n",
      "Epoch: 9, Train Loss: 0.8117, Val Loss: 7.2246\n",
      "Epoch: 10, Train Loss: 0.7193, Val Loss: 6.8318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 데이터 로드 및 처리\n",
    "users, movies, ratings = [], [], []\n",
    "user_len, movie_len = 0, 0\n",
    "\n",
    "with open(\"data/ml-1m/ratings.dat\", \"r\") as f:\n",
    "    for line in f:\n",
    "        u, m, r, t = line.split(\"::\")\n",
    "        user_len = max(user_len, int(u))\n",
    "        movie_len = max(movie_len, int(m))\n",
    "        users.append(int(u))\n",
    "        movies.append(int(m))\n",
    "        ratings.append(float(r))  # float 유지!\n",
    "\n",
    "# 데이터 섞기\n",
    "indices = np.arange(len(ratings))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "users = np.array(users)[indices]\n",
    "movies = np.array(movies)[indices]\n",
    "ratings = np.array(ratings)[indices]\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "users = torch.tensor(users, dtype=torch.long, device=device)\n",
    "movies = torch.tensor(movies, dtype=torch.long, device=device)\n",
    "ratings = torch.tensor(ratings, dtype=torch.float, device=device)  # float로 변환\n",
    "\n",
    "# 데이터 분할 (60% 학습, 20% 검증, 20% 테스트)\n",
    "n_samples = len(ratings)\n",
    "train_size = int(0.6 * n_samples)\n",
    "valid_size = int(0.2 * n_samples)\n",
    "\n",
    "users_train, movies_train, ratings_train = users[:train_size], movies[:train_size], ratings[:train_size]\n",
    "users_valid, movies_valid, ratings_valid = users[train_size:train_size+valid_size], movies[train_size:train_size+valid_size], ratings[train_size:train_size+valid_size]\n",
    "users_test, movies_test, ratings_test = users[train_size+valid_size:], movies[train_size+valid_size:], ratings[train_size+valid_size:]\n",
    "\n",
    "# DataLoader 활용\n",
    "batch_size = 64\n",
    "train_data = DataLoader(TensorDataset(users_train, movies_train, ratings_train), batch_size=batch_size, shuffle=True)\n",
    "valid_data = DataLoader(TensorDataset(users_valid, movies_valid, ratings_valid), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, user_len, movie_len, rank):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_bias = nn.Embedding(user_len+1, 1)\n",
    "        self.item_bias = nn.Embedding(movie_len+1, 1)\n",
    "        self.user_reps = nn.Embedding(user_len+1, rank)\n",
    "        self.item_reps = nn.Embedding(movie_len+1, rank)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_bias = self.user_bias(user).squeeze()\n",
    "        item_bias = self.item_bias(item).squeeze()\n",
    "        user_emb = self.user_reps(user)\n",
    "        item_emb = self.item_reps(item)\n",
    "        return user_bias + item_bias + torch.sum(user_emb * item_emb, dim=1)\n",
    "\n",
    "# 학습 설정\n",
    "rank = 100\n",
    "epochs = 10\n",
    "\n",
    "model = MF(user_len, movie_len, rank).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for user, movie, rating in train_data:\n",
    "        user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
    "\n",
    "        pred = model(user, movie)\n",
    "        loss = criterion(pred, rating)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    with torch.no_grad():\n",
    "        for user, movie, rating in valid_data:\n",
    "            user, movie, rating = user.to(device), movie.to(device), rating.to(device)\n",
    "            pred = model(user, movie)\n",
    "            loss = criterion(pred, rating)\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "    # 로그 출력\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {np.mean(train_losses):.4f}, Val Loss: {np.mean(valid_losses):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3802], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user = users_test[3].unsqueeze(0)\n",
    "    movie = movies_test[3].unsqueeze(0)\n",
    "    ratings = ratings_test[3].unsqueeze(0)\n",
    "    pred = model(user, movie)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
