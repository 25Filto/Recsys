{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def load_data_1m( delimiter='::', frac=0.1, seed=1234):\n",
    "    print('reading data...')\n",
    "    df = pd.read_csv(\"../data/ratings.dat\", delimiter=delimiter, engine='python', header=None)\n",
    "    data = df.to_numpy().astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
    "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data matrix loaded\n",
      "num of users: 6040\n",
      "num of movies: 3706\n",
      "num of training ratings: 900189\n",
      "num of test ratings: 100020\n"
     ]
    }
   ],
   "source": [
    "n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(delimiter='::', frac=0.1, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500 # size of hidden layers\n",
    "n_dim = 5 # inner AE embedding size\n",
    "n_layers = 2 # number of hidden layers\n",
    "gk_size = 3 # width=height of kernel for convolution\n",
    "\n",
    "# Hyperparameters to tune for specific case\n",
    "max_epoch_p = 500 # max number of epochs for pretraining\n",
    "max_epoch_f = 1000 # max number of epochs for finetuning\n",
    "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10 # and finetuning\n",
    "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-5 # and finetuning\n",
    "lambda_2 = 20. # regularisation of number or parameters\n",
    "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1 # dot product weight for global kernel\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "def local_kernel(u, v):\n",
    "    dist = torch.norm(u - v, p=2, dim=2)\n",
    "    hat = torch.clamp(1. - dist**2, min=0.)\n",
    "    return hat\n",
    "\n",
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
    "      super().__init__()\n",
    "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
    "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
    "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
    "      self.b = nn.Parameter(torch.randn(n_hid))\n",
    "\n",
    "      self.lambda_s = lambda_s\n",
    "      self.lambda_2 = lambda_2\n",
    "\n",
    "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.zeros_(self.b)\n",
    "      self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "      w_hat = local_kernel(self.u, self.v)\n",
    "    \n",
    "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
    "      sparse_reg_term = self.lambda_s * sparse_reg\n",
    "      \n",
    "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
    "      l2_reg_term = self.lambda_2 * l2_reg\n",
    "\n",
    "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
    "      y = torch.matmul(x, W_eff) + self.b\n",
    "      y = self.activation(y)\n",
    "\n",
    "      return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
    "      super().__init__()\n",
    "      layers = []\n",
    "      for i in range(n_layers):\n",
    "        if i == 0:\n",
    "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
    "        else:\n",
    "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
    "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
    "      self.layers = nn.ModuleList(layers)\n",
    "      self.dropout = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x):\n",
    "      total_reg = None\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        x, reg = layer(x)\n",
    "        if i < len(self.layers)-1:\n",
    "          x = self.dropout(x)\n",
    "        if total_reg is None:\n",
    "          total_reg = reg\n",
    "        else:\n",
    "          total_reg += reg\n",
    "      return x, total_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CompleteNet(nn.Module):\n",
    "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
    "      super().__init__()\n",
    "      self.gk_size = gk_size\n",
    "      self.dot_scale = dot_scale\n",
    "      self.local_kernel_net = kernel_net\n",
    "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
    "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      \n",
    "\n",
    "    def forward(self, x, x_local):\n",
    "      gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
    "      x = self.global_conv(x, gk)\n",
    "      x, global_reg_loss = self.local_kernel_net(x)\n",
    "      return x, global_reg_loss\n",
    "\n",
    "    def global_kernel(self, input, gk_size, dot_scale):\n",
    "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
    "      avg_pooling = avg_pooling.view(1, -1)\n",
    "\n",
    "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
    "      gk = gk.view(1, 1, gk_size, gk_size)\n",
    "\n",
    "      return gk\n",
    "\n",
    "    def global_conv(self, input, W):\n",
    "      input = input.unsqueeze(0).unsqueeze(0)\n",
    "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
    "      return conv2d.squeeze(0).squeeze(0)\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
    "      # L2 loss\n",
    "      diff = train_m * (train_r - pred_p)\n",
    "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
    "      loss_p = sqE + reg_loss\n",
    "      return loss_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE-TRAINING\n",
      "Epoch: 0 test rmse: 2.8171895 train rmse: 2.8120039\n",
      "PRE-TRAINING\n",
      "Epoch: 14 test rmse: 1.5194974 train rmse: 1.5205328\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  x = torch.Tensor(train_r).float().to(device)\n",
    "  m = torch.Tensor(train_m).float().to(device)\n",
    "  complete_model.local_kernel_net.train()\n",
    "  pred, reg = complete_model.local_kernel_net(x)\n",
    "  loss = Loss().to(device)(pred, reg, m, x)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_p):\n",
    "  optimizer.step(closure)\n",
    "  complete_model.local_kernel_net.eval()\n",
    "\n",
    "  pre, _ = model(torch.Tensor(train_r).float().to(device))\n",
    "  \n",
    "  pre = pre.float().cpu().detach().numpy()\n",
    "  \n",
    "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "  test_rmse = np.sqrt(error)\n",
    "\n",
    "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "  train_rmse = np.sqrt(error_train)\n",
    "\n",
    "  if last_rmse-train_rmse < tol_p:\n",
    "    counter += 1\n",
    "  else:\n",
    "    counter = 0\n",
    "\n",
    "  last_rmse = train_rmse\n",
    "\n",
    "  if patience_p == counter:\n",
    "    print('PRE-TRAINING')\n",
    "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    " \n",
    "    break\n",
    "\n",
    "\n",
    "  if i % 50 != 0:\n",
    "    continue\n",
    "  print('PRE-TRAINING')\n",
    "  print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNING\n",
      "Epoch: 0 test rmse: 1.2154596\n",
      "Epoch: 0 train rmse: 1.2128808\n",
      "FINE-TUNING\n",
      "Epoch: 50 test rmse: 0.91980827\n",
      "Epoch: 50 train rmse: 0.9115928\n",
      "FINE-TUNING\n",
      "Epoch: 100 test rmse: 0.90192175\n",
      "Epoch: 100 train rmse: 0.8918924\n",
      "FINE-TUNING\n",
      "Epoch: 150 test rmse: 0.89579695\n",
      "Epoch: 150 train rmse: 0.8850557\n",
      "FINE-TUNING\n",
      "Epoch: 200 test rmse: 0.8953099\n",
      "Epoch: 200 train rmse: 0.88470805\n",
      "FINE-TUNING\n",
      "Epoch: 250 test rmse: 0.8945604\n",
      "Epoch: 250 train rmse: 0.88408995\n",
      "FINE-TUNING\n",
      "Epoch: 300 test rmse: 0.8945244\n",
      "Epoch: 300 train rmse: 0.8837347\n",
      "FINE-TUNING\n",
      "Epoch: 350 test rmse: 0.89357525\n",
      "Epoch: 350 train rmse: 0.882469\n",
      "FINE-TUNING\n",
      "Epoch: 400 test rmse: 0.892964\n",
      "Epoch: 400 train rmse: 0.8816494\n",
      "FINE-TUNING\n",
      "Epoch: 450 test rmse: 0.8923989\n",
      "Epoch: 450 train rmse: 0.8807853\n",
      "FINE-TUNING\n",
      "Epoch: 500 test rmse: 0.89199096\n",
      "Epoch: 500 train rmse: 0.88035834\n",
      "FINE-TUNING\n",
      "Epoch: 550 test rmse: 0.89108175\n",
      "Epoch: 550 train rmse: 0.8793578\n",
      "FINE-TUNING\n",
      "Epoch: 600 test rmse: 0.890266\n",
      "Epoch: 600 train rmse: 0.87835526\n",
      "FINE-TUNING\n",
      "Epoch: 650 test rmse: 0.88923347\n",
      "Epoch: 650 train rmse: 0.87695974\n",
      "FINE-TUNING\n",
      "Epoch: 700 test rmse: 0.8881241\n",
      "Epoch: 700 train rmse: 0.8758312\n",
      "FINE-TUNING\n",
      "Epoch: 750 test rmse: 0.88725954\n",
      "Epoch: 750 train rmse: 0.8747727\n",
      "FINE-TUNING\n",
      "Epoch: 800 test rmse: 0.88665605\n",
      "Epoch: 800 train rmse: 0.8736503\n",
      "FINE-TUNING\n",
      "Epoch: 850 test rmse: 0.8846311\n",
      "Epoch: 850 train rmse: 0.87142044\n",
      "FINE-TUNING\n",
      "Epoch: 900 test rmse: 0.8834365\n",
      "Epoch: 900 train rmse: 0.87002134\n",
      "FINE-TUNING\n",
      "Epoch: 950 test rmse: 0.88265204\n",
      "Epoch: 950 train rmse: 0.868995\n"
     ]
    }
   ],
   "source": [
    "train_r_local = np.clip(pre, 1., 5.)\n",
    "\n",
    "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  x = torch.Tensor(train_r).float().to(device)\n",
    "  x_local = torch.Tensor(train_r_local).float().to(device)\n",
    "  m = torch.Tensor(train_m).float().to(device)\n",
    "  complete_model.train()\n",
    "  pred, reg = complete_model(x, x_local)\n",
    "  loss = Loss().to(device)(pred, reg, m, x)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_f):\n",
    "  optimizer.step(closure)\n",
    "  complete_model.eval()\n",
    "\n",
    "\n",
    "  pre, _ = complete_model(torch.Tensor(train_r).float().to(device), torch.Tensor(train_r_local).float().to(device))\n",
    "  \n",
    "  pre = pre.float().cpu().detach().numpy()\n",
    "\n",
    "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "  test_rmse = np.sqrt(error)\n",
    "\n",
    "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "  train_rmse = np.sqrt(error_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  if last_rmse-train_rmse < tol_f:\n",
    "    counter += 1\n",
    "  else:\n",
    "    counter = 0\n",
    "\n",
    "  last_rmse = train_rmse\n",
    "\n",
    "  if patience_f == counter:\n",
    "    print('FINE-TUNING')\n",
    "    print('Epoch:', i+1, 'test rmse:', test_rmse)\n",
    "    print('Epoch:', i+1, 'train rmse:', train_rmse)\n",
    "    break\n",
    "\n",
    "\n",
    "  if i % 50 != 0:\n",
    "    continue\n",
    "\n",
    "  print('FINE-TUNING')\n",
    "  print('Epoch:', i, 'test rmse:', test_rmse)\n",
    "  print('Epoch:', i, 'train rmse:', train_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
